{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L_mc7y9B9W6J"
      },
      "outputs": [],
      "source": [
        "#API Key laden\n",
        "#OpenAI-API-Schlüssel sicher aus gespeicherten Colab-Benutzerdaten in Notebook schreiben\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('apikey_11')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Installation\n",
        "#LangChain-Integrationspaket für OpenAI installieren, um OpenAI-Modelle direkt in LangChain zu nutzen\n",
        "# ! -> in Shell ausgeführt\n",
        "!pip install langchain_openai"
      ],
      "metadata": {
        "id": "hxnazpp1DaUW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Installation\n",
        "#LangChain-Paket mit Schnittstellen zu externen Tools wie z.b. PyPDFLoader\n",
        "# ! -> in Shell ausgeführt\n",
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "I6HBYXDyesI5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "#Text-Splitter aus LangChain-Bibliothek\n",
        "#Teilt lange Texte in Chunks, um Embeddings zu berechnen\n",
        "#Geht Rekursiv vor: zuerst Absätze, dann Sätze, dann Zeichen\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "#OpenAIEmbeddings ist die Schnittstelle um OpenAI-Modelle für Embeddings zu nutzen\n",
        "#ChatOpenAI ist die Schnittstelle zu den OpenAI-Chatmodellen\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "\n",
        "#FAISS ist eine Vektor-Datenbank von Meta, die Embeddings speichert\n",
        "#FAISS läuft lokal\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "#PyPDFLoader übersetzt PDFs zu LangChain-Dokumenten zur Weiterverarbeitung\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "#Chain ist die kombinierte Pipeline in LangChain\n",
        "#RetrievalQA umfasst den vollständigen Ablauf des RAGs und übernimmt somit das Zusammenspiel zwischen den Komponenten\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "kjxaeHS7ekul"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Installation\n",
        "#Python-Bibliothek zum Arbeiten mit PDF-Dateien\n",
        "# ! -> in Shell ausgeführt\n",
        "!pip install pypdf"
      ],
      "metadata": {
        "collapsed": true,
        "id": "F60tJ0VPkfKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#externe Quelle (PDF)\n",
        "import requests\n",
        "\n",
        "# URL, wo das PDF abliegt\n",
        "url = \"https://www.h-kronenberg.ch/obst_wein_schnaps_essig/Anleitungen/Wein%20Herstellung.pdf\"\n",
        "\n",
        "#Hier wird ein loader für die PDF erstellt\n",
        "loader = PyPDFLoader(url)\n",
        "#Der loader extrahiert den Text aus dem PDF aufgeteilt nach Seiten und speichert das Ergebnis in docs.\n",
        "#docs ist eine Liste von LangChain-Dokumenten für jeweils jede Seite\n",
        "#Jedes Dokument enthält den Inhalt und Metadaten für die jeweilige Seite\n",
        "docs = loader.load()\n"
      ],
      "metadata": {
        "id": "K4bKLEALDeE4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chunking\n",
        "\n",
        "#chunk_size -> wie groß sollen Chunks sein\n",
        "#chunk_overlap -> wie viel sollen die Chunks sich überschneiden, um keinen Kontext zu verlieren\n",
        "# hier: Jeder Abschnitt überlappt sich mit dem vorherigen um 30 Zeichen\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=30)\n",
        "#Liste docs wird mit denkleineren Chunks überschrieben\n",
        "docs = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "aZuHOb0OkGEX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Installation\n",
        "\n",
        "#Python-Paket von Pypl\n",
        "#FAISS = Facebook AI Similarity Search\n",
        "#--> Bibliothek von Meta zur Vektoren- und Ähnlichkeitssuche\n",
        "# ! -> in Shell ausgeführt\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "6IX7-dC2kBRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Embedding + Vektordatenbank (FAISS)\n",
        "\n",
        "#OpenAIEmbeddings ist die Schnittstelle von LangChain zu OpenAI-Embedding-Modellen\n",
        "#openai_api_key --> API-Schlüssel, damit LangChain die OpenAI-API nutzen darf\n",
        "#model --> das Modell, das OpenAI für Embeddings bereitstellt; günstiger und schneller als text-embedding-3-large\n",
        "#embeddings kann beliebige Texte in semantische Vektoren umwandeln\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\")\n",
        "#Jedes Dokument aus Liste docs wird mit Embeddings in Embedding-Vektor umgewandelt\n",
        "#Embedding-Vektoren werden in FAISS abgelegt\n",
        "#vectorstore enthält die Embedding-Vektoren\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "pCLIJm6tjupt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Retriever\n",
        "\n",
        "#aus vectorstore wird ein Retriever gemacht (as_retriever), als Schnittstelle für die semantische Suche\n",
        "#retiever kann von LangChain in QA-Chains verwendet werden\n",
        "#search_type=\"similarity\" --> wie soll gesucht werden\n",
        "#search_kwargs={\"k\": 3} --> gib 3 Chunks zurück\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "HLbj6xw6jvRk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LLM\n",
        "\n",
        "#ChatOpenAI --> Schnittstelle von LangChain zu OpenAI\n",
        "#openai_api_key --> Schlüssel, damit LangChain auf Modell zugreifen kann\n",
        "#model ist das Sprachmodell das bei OpenAI verwendet wird\n",
        "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "hl3hL_O0jxMV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pipeline: Retrieval + QA-Chain\n",
        "\n",
        "#QA_Chain ist die komplette Pipeline zusammengefasse\n",
        "#RetrievalQA ist fertige Chain in LangChain, die Retrieval und LLM kombiniert\n",
        "#from_chain_type baut die QA-Pipeline auf, damit nicht jeder Schritt manuell zusammengestellt werden muss\n",
        "#Aus dem vorherigen Schitten wird das LLM und der Retriever übergeben\n",
        "#chain_type definiert, wie gefundenen Chunks an LLM übergeben werden\n",
        "#--> \"stuff\" -> alle relevanten Dokumente werden einfach in das LLM gegeben\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    chain_type=\"stuff\"\n",
        ")"
      ],
      "metadata": {
        "id": "mWqPOZfwj0wE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testanfrage\n",
        "query = \"Auf was muss man besonders bei der Herstellung von Wein achten?\"\n",
        "\n",
        "#die Pipeline wird aufgerufen mit der query und gibt die Antowrt zurück\n",
        "response = qa_chain.invoke(query)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq4ZIfU0j1mN",
        "outputId": "d8f4342e-5777-489d-cf0b-a02131e7dec5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'Auf was muss man besonders bei der Herstellung von Wein achten?', 'result': 'Bei der Herstellung von Wein aus Weintrauben ist besonders darauf zu achten, dass die Trauben sorgfältig behandelt werden, um die Qualität des Weines zu gewährleisten.'}\n"
          ]
        }
      ]
    }
  ]
}
